{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity-embedding-rossmann \n",
    "\n",
    "This is a Pytorch implementation with sklearn model interface for which most DS are familiar (`model.fit(X, y)`  and `model.predict(X, y)`)\n",
    "\n",
    "This implementation reproduces the code used in the paper **\"[Entity Embeddings of Categorical Variables](http://arxiv.org/abs/1604.06737)\"** and extends its functionality to other Machine Learning problems. \n",
    "\n",
    "The original Keras code used as a benchmark can be found in: \n",
    "**Entity Embeddings of Categorical Variables [REPO](https://github.com/entron/entity-embedding-rossmann)**.\n",
    "\n",
    "# Notes\n",
    "\n",
    "This repo aims to provide an Entity Embedding Neural Network out-of-the-box model for Regression and Classification tasks.\n",
    "\n",
    "To this date this repo has implemented:\n",
    "\n",
    "- Regression (tested on original implementation in here).\n",
    "- Binary Classification (used `EntEmbNNBinary` instead of `EntEmbNNRegression`) (tested on personal projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import eval_utils\n",
    "import numpy as np\n",
    "\n",
    "from EENNRegression import EntEmbNNRegression\n",
    "\n",
    "X, y, X_test, y_test = datasets.get_X_train_test_data()\n",
    "\n",
    "# This normalization comes from original Entity Emb. original Code\n",
    "y_max = max(y.max(), y_test.max())\n",
    "y = np.log(y) / np.log(y_max)\n",
    "y_test = np.log(y_test) / np.log(y_max)\n",
    "\n",
    "for data in [X, X_test]:\n",
    "    data.drop('Open', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[1] Test: MSE:0.000197 MAE: 0.010257 gini: 0.901585 R2: 0.878147 MAPE: 0.012573\n",
      "\t[2] Test: MSE:0.000156 MAE: 0.00897 gini: 0.890218 R2: 0.903692 MAPE: 0.011022\n",
      "\t[3] Test: MSE:0.000146 MAE: 0.008785 gini: 0.916184 R2: 0.909749 MAPE: 0.010714\n",
      "\t[4] Test: MSE:0.000157 MAE: 0.00908 gini: 0.890544 R2: 0.902871 MAPE: 0.011217\n",
      "\t[5] Test: MSE:0.000132 MAE: 0.008313 gini: 0.918975 R2: 0.918285 MAPE: 0.010129\n",
      "\t[6] Test: MSE:0.000122 MAE: 0.007781 gini: 0.921821 R2: 0.924628 MAPE: 0.009557\n",
      "\t[7] Test: MSE:0.000157 MAE: 0.009234 gini: 0.907448 R2: 0.903 MAPE: 0.011397\n",
      "\t[8] Test: MSE:0.000115 MAE: 0.007533 gini: 0.93635 R2: 0.929313 MAPE: 0.009253\n",
      "\t[9] Test: MSE:0.000116 MAE: 0.007687 gini: 0.933982 R2: 0.928413 MAPE: 0.009381\n",
      "\t[10] Test: MSE:0.000107 MAE: 0.0073 gini: 0.950313 R2: 0.933775 MAPE: 0.008926\n",
      "\n",
      "\n",
      "\t[1] Test: MSE:0.000313 MAE: 0.013938 gini: 0.894436 R2: 0.807053 MAPE: 0.016888\n",
      "\t[2] Test: MSE:0.00016 MAE: 0.009192 gini: 0.937388 R2: 0.901161 MAPE: 0.011281\n",
      "\t[3] Test: MSE:0.000161 MAE: 0.009495 gini: 0.909457 R2: 0.900399 MAPE: 0.011533\n",
      "\t[4] Test: MSE:0.000182 MAE: 0.010022 gini: 0.933888 R2: 0.887558 MAPE: 0.012314\n",
      "\t[5] Test: MSE:0.000147 MAE: 0.008902 gini: 0.946892 R2: 0.909089 MAPE: 0.010856\n",
      "\t[6] Test: MSE:0.000115 MAE: 0.007516 gini: 0.95848 R2: 0.929167 MAPE: 0.009215\n",
      "\t[7] Test: MSE:0.000152 MAE: 0.009242 gini: 0.998578 R2: 0.906032 MAPE: 0.011301\n",
      "\t[8] Test: MSE:0.000164 MAE: 0.009938 gini: 0.925367 R2: 0.898473 MAPE: 0.012055\n",
      "\t[9] Test: MSE:0.000112 MAE: 0.007541 gini: 0.918505 R2: 0.930832 MAPE: 0.00921\n",
      "\t[10] Test: MSE:0.000109 MAE: 0.007328 gini: 0.928719 R2: 0.932866 MAPE: 0.008957\n",
      "\n",
      "\n",
      "\t[1] Test: MSE:0.000424 MAE: 0.017035 gini: 0.939391 R2: 0.738143 MAPE: 0.020684\n",
      "\t[2] Test: MSE:0.000166 MAE: 0.009495 gini: 0.881094 R2: 0.897251 MAPE: 0.011547\n",
      "\t[3] Test: MSE:0.000164 MAE: 0.009542 gini: 0.887316 R2: 0.898718 MAPE: 0.011584\n",
      "\t[4] Test: MSE:0.000127 MAE: 0.007968 gini: 0.961209 R2: 0.921612 MAPE: 0.009771\n",
      "\t[5] Test: MSE:0.000156 MAE: 0.009121 gini: 0.897298 R2: 0.903945 MAPE: 0.011263\n",
      "\t[6] Test: MSE:0.000118 MAE: 0.007692 gini: 0.933346 R2: 0.927043 MAPE: 0.009455\n",
      "\t[7] Test: MSE:0.000182 MAE: 0.01033 gini: 0.936644 R2: 0.88761 MAPE: 0.0127\n",
      "\t[8] Test: MSE:0.000106 MAE: 0.007144 gini: 0.925808 R2: 0.934805 MAPE: 0.008759\n",
      "\t[9] Test: MSE:0.00012 MAE: 0.00797 gini: 0.935092 R2: 0.926016 MAPE: 0.009718\n",
      "\t[10] Test: MSE:0.000103 MAE: 0.007038 gini: 0.96785 R2: 0.936545 MAPE: 0.008631\n",
      "\n",
      "\n",
      "\t[1] Test: MSE:0.000191 MAE: 0.010043 gini: 0.930669 R2: 0.881849 MAPE: 0.012285\n",
      "\t[2] Test: MSE:0.000239 MAE: 0.012081 gini: 0.912542 R2: 0.85227 MAPE: 0.014648\n",
      "\t[3] Test: MSE:0.000153 MAE: 0.008996 gini: 0.916455 R2: 0.905708 MAPE: 0.010968\n",
      "\t[4] Test: MSE:0.000139 MAE: 0.008431 gini: 0.902012 R2: 0.914255 MAPE: 0.010304\n",
      "\t[5] Test: MSE:0.000135 MAE: 0.008451 gini: 0.911053 R2: 0.916404 MAPE: 0.010294\n",
      "\t[6] Test: MSE:0.000129 MAE: 0.008114 gini: 0.939414 R2: 0.920345 MAPE: 0.009977\n",
      "\t[7] Test: MSE:0.000231 MAE: 0.012297 gini: 0.957719 R2: 0.857281 MAPE: 0.015055\n",
      "\t[8] Test: MSE:0.000113 MAE: 0.00748 gini: 0.993521 R2: 0.930126 MAPE: 0.009168\n",
      "\t[9] Test: MSE:0.000122 MAE: 0.008006 gini: 0.894544 R2: 0.924937 MAPE: 0.009735\n",
      "\t[10] Test: MSE:0.000126 MAE: 0.008209 gini: 0.902523 R2: 0.922084 MAPE: 0.009974\n",
      "\n",
      "\n",
      "\t[1] Test: MSE:0.000202 MAE: 0.010387 gini: 0.918051 R2: 0.875259 MAPE: 0.012679\n",
      "\t[2] Test: MSE:0.00015 MAE: 0.008777 gini: 0.907705 R2: 0.907646 MAPE: 0.010736\n",
      "\t[3] Test: MSE:0.000153 MAE: 0.009069 gini: 0.935907 R2: 0.905336 MAPE: 0.011057\n",
      "\t[4] Test: MSE:0.000203 MAE: 0.011176 gini: 0.950277 R2: 0.874687 MAPE: 0.013593\n",
      "\t[5] Test: MSE:0.000162 MAE: 0.00959 gini: 0.944633 R2: 0.899964 MAPE: 0.011673\n",
      "\t[6] Test: MSE:0.000132 MAE: 0.008294 gini: 0.953197 R2: 0.918346 MAPE: 0.010186\n",
      "\t[7] Test: MSE:0.000142 MAE: 0.00872 gini: 0.932828 R2: 0.912403 MAPE: 0.010734\n",
      "\t[8] Test: MSE:0.000122 MAE: 0.00792 gini: 0.943742 R2: 0.924461 MAPE: 0.009741\n",
      "\t[9] Test: MSE:0.000108 MAE: 0.007291 gini: 0.902824 R2: 0.933326 MAPE: 0.008921\n",
      "\t[10] Test: MSE:0.000106 MAE: 0.007158 gini: 0.912962 R2: 0.934381 MAPE: 0.008805\n",
      "\n",
      "\n",
      "MAPE: 0.010769100501334554\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for _ in range(5):\n",
    "    m = EntEmbNNRegression(\n",
    "        cat_emb_dim={\n",
    "            'Store': 10,\n",
    "            'DayOfWeek': 6,\n",
    "            'Promo': 1,\n",
    "            'Year': 2,\n",
    "            'Month': 6,\n",
    "            'Day': 10,\n",
    "            'State': 6},\n",
    "        alpha=0,\n",
    "        dense_layers=[1000, 500],\n",
    "        drop_out_layers=[0., 0.],\n",
    "        drop_out_emb=0.,\n",
    "        loss_function='L1Loss',\n",
    "        train_size=1., \n",
    "        verbose=True)\n",
    "\n",
    "    m.fit(X, y)\n",
    "    models.append(m)\n",
    "    print('\\n')\n",
    "\n",
    "test_y_pred = np.array([model.predict(X_test) for model in models])\n",
    "test_y_pred = test_y_pred.mean(axis=0)\n",
    "\n",
    "print('MAPE: %s' % eval_utils.MAPE(\n",
    "    y_true=y_test.values.flatten(),\n",
    "    y_pred=test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Original output from [REPO](https://github.com/entron/entity-embedding-rossmann) code**:\n",
    "    \n",
    "`\n",
    "Using TensorFlow backend.\n",
    "Number of samples used for training: 200000\n",
    "Fitting NN_with_EntityEmbedding...\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 64us/step - loss: 0.0142 - val_loss: 0.0119\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0096 - val_loss: 0.0109\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0089 - val_loss: 0.0113\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0082 - val_loss: 0.0101\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0077 - val_loss: 0.0101\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0074 - val_loss: 0.0100\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0072 - val_loss: 0.0099\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 59us/step - loss: 0.0071 - val_loss: 0.0096\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0069 - val_loss: 0.0092\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0068 - val_loss: 0.0095\n",
    "Result on validation data:  0.10152226095724903\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 63us/step - loss: 0.0140 - val_loss: 0.0117\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0093 - val_loss: 0.0107\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0084 - val_loss: 0.0109\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0079 - val_loss: 0.0096\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0076 - val_loss: 0.0097\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0074 - val_loss: 0.0097\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0072 - val_loss: 0.0097\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0093\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0069 - val_loss: 0.0094\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0068 - val_loss: 0.0093\n",
    "Result on validation data:  0.10194501967184522\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 64us/step - loss: 0.0141 - val_loss: 0.0121\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0093 - val_loss: 0.0100\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0084 - val_loss: 0.0098\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0079 - val_loss: 0.0095\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0076 - val_loss: 0.0098\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0074 - val_loss: 0.0097\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0072 - val_loss: 0.0098\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0071 - val_loss: 0.0092\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0093\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0069 - val_loss: 0.0097\n",
    "Result on validation data:  0.10076855799458961\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0141 - val_loss: 0.0114\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0093 - val_loss: 0.0105\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0084 - val_loss: 0.0108\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0079 - val_loss: 0.0099\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0076 - val_loss: 0.0098\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0074 - val_loss: 0.0099\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0072 - val_loss: 0.0100\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0071 - val_loss: 0.0095\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0096\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0068 - val_loss: 0.0099\n",
    "Result on validation data:  0.10973501886112967\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 63us/step - loss: 0.0144 - val_loss: 0.0116\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0094 - val_loss: 0.0109\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0084 - val_loss: 0.0103\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0079 - val_loss: 0.0099\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0076 - val_loss: 0.0104\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0074 - val_loss: 0.0099\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0072 - val_loss: 0.0099\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0099\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0069 - val_loss: 0.0097\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0068 - val_loss: 0.0100\n",
    "Result on validation data:  0.10491748954856149\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGB performance**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.14712066617861289\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X_train, y_train, X_test, y_test = datasets.get_X_train_test_data()\n",
    "dtrain = xgb.DMatrix(\n",
    "    X_train.apply(lambda x: x.cat.codes),\n",
    "    label=np.log(y_train))\n",
    "evallist = [(dtrain, 'train')]\n",
    "param = {'nthread': 12,\n",
    "         'max_depth': 7,\n",
    "         'eta': 0.02,\n",
    "         'silent': 1,\n",
    "         'objective': 'reg:linear',\n",
    "         'colsample_bytree': 0.7,\n",
    "         'subsample': 0.7}\n",
    "\n",
    "num_round = 3000\n",
    "bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n",
    "\n",
    "xgb_test_y_pred = bst.predict(\n",
    "    xgb.DMatrix(X_test.apply(lambda x: x.cat.codes))\n",
    ")\n",
    "xgb_test_y_pred = np.exp((xgb_test_y_pred))\n",
    "print('MAPE: %s' % eval_utils.MAPE(\n",
    "    y_true=y_test, \n",
    "    y_pred=xgb_test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Original output from [REPO](https://github.com/entron/entity-embedding-rossmann) code**:\n",
    "\n",
    "`\n",
    ".\n",
    ".\n",
    ".\n",
    "[2987]  train-rmse:0.148847\n",
    "[2988]  train-rmse:0.148845\n",
    "[2989]  train-rmse:0.148842\n",
    "[2990]  train-rmse:0.148839\n",
    "[2991]  train-rmse:0.148834\n",
    "[2992]  train-rmse:0.148819\n",
    "[2993]  train-rmse:0.148768\n",
    "[2994]  train-rmse:0.148762\n",
    "[2995]  train-rmse:0.148741\n",
    "[2996]  train-rmse:0.148705\n",
    "[2997]  train-rmse:0.148667\n",
    "[2998]  train-rmse:0.148622\n",
    "[2999]  train-rmse:0.148584\n",
    "Result on validation data:  0.14691216270195093\n",
    "`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
