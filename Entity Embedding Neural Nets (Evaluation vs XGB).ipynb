{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity-embedding-rossmann \n",
    "\n",
    "This is a Pytorch implementation with sklearn model interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raul/embeddings/EntEmbNN.py:397: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  self.log_losses.append(loss.data[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[1] loss_train:0.00951 loss_test:0.01147\n",
      "\t[2] loss_train:0.00884 loss_test:0.01089\n",
      "\t[3] loss_train:0.00775 loss_test:0.01016\n",
      "\t[4] loss_train:0.00729 loss_test:0.01016\n",
      "\t[5] loss_train:0.00737 loss_test:0.00996\n",
      "\t[6] loss_train:0.00693 loss_test:0.00966\n",
      "\t[7] loss_train:0.00754 loss_test:0.01088\n",
      "\t[8] loss_train:0.0065 loss_test:0.0102\n",
      "\t[9] loss_train:0.00675 loss_test:0.00948\n",
      "\t[10] loss_train:0.00654 loss_test:0.00964\n",
      "\n",
      "\n",
      "\t[1] loss_train:0.00938 loss_test:0.01126\n",
      "\t[2] loss_train:0.00853 loss_test:0.01071\n",
      "\t[3] loss_train:0.00836 loss_test:0.01147\n",
      "\t[4] loss_train:0.00754 loss_test:0.01061\n",
      "\t[5] loss_train:0.00728 loss_test:0.00996\n",
      "\t[6] loss_train:0.00742 loss_test:0.0099\n",
      "\t[7] loss_train:0.00662 loss_test:0.00962\n",
      "\t[8] loss_train:0.00662 loss_test:0.00992\n",
      "\t[9] loss_train:0.00662 loss_test:0.00956\n",
      "\t[10] loss_train:0.00688 loss_test:0.01009\n",
      "\n",
      "\n",
      "\t[1] loss_train:0.00964 loss_test:0.01127\n",
      "\t[2] loss_train:0.00957 loss_test:0.01116\n",
      "\t[3] loss_train:0.0086 loss_test:0.01167\n",
      "\t[4] loss_train:0.00742 loss_test:0.01032\n",
      "\t[5] loss_train:0.00724 loss_test:0.01058\n",
      "\t[6] loss_train:0.00709 loss_test:0.00985\n",
      "\t[7] loss_train:0.00688 loss_test:0.01003\n",
      "\t[8] loss_train:0.00666 loss_test:0.01054\n",
      "\t[9] loss_train:0.00649 loss_test:0.01036\n",
      "\t[10] loss_train:0.00663 loss_test:0.01083\n",
      "\n",
      "\n",
      "\t[1] loss_train:0.00982 loss_test:0.01154\n",
      "\t[2] loss_train:0.00886 loss_test:0.01025\n",
      "\t[3] loss_train:0.00782 loss_test:0.01089\n",
      "\t[4] loss_train:0.00726 loss_test:0.00991\n",
      "\t[5] loss_train:0.00738 loss_test:0.01071\n",
      "\t[6] loss_train:0.00839 loss_test:0.01166\n",
      "\t[7] loss_train:0.00707 loss_test:0.00955\n",
      "\t[8] loss_train:0.00666 loss_test:0.00992\n",
      "\t[9] loss_train:0.0065 loss_test:0.00924\n",
      "\t[10] loss_train:0.00669 loss_test:0.00995\n",
      "\n",
      "\n",
      "\t[1] loss_train:0.01091 loss_test:0.01317\n",
      "\t[2] loss_train:0.00833 loss_test:0.0107\n",
      "\t[3] loss_train:0.00771 loss_test:0.01018\n",
      "\t[4] loss_train:0.00752 loss_test:0.01016\n",
      "\t[5] loss_train:0.00732 loss_test:0.00999\n",
      "\t[6] loss_train:0.00687 loss_test:0.00976\n",
      "\t[7] loss_train:0.00675 loss_test:0.00987\n",
      "\t[8] loss_train:0.00655 loss_test:0.00937\n",
      "\t[9] loss_train:0.00653 loss_test:0.00942\n",
      "\t[10] loss_train:0.00729 loss_test:0.00966\n",
      "\n",
      "\n",
      "MAPE: 0.09726746216202672\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import eval_utils\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "import EntEmbNN as eenn\n",
    "\n",
    "X_train, y_train, X_test, y_test = datasets.get_X_train_test_data()\n",
    "\n",
    "for data in [X_train, X_test]:\n",
    "    data.drop('Open', inplace=True, axis=1)\n",
    "\n",
    "models = []\n",
    "for _ in range(5):\n",
    "    m = eenn.EntEmbNNRegression(\n",
    "        X_y_test = (X_test, y_test),\n",
    "        cat_emb_dim={\n",
    "            'Store': 10,\n",
    "            'DayOfWeek': 6,\n",
    "            'Promo': 1,\n",
    "            'Year': 2,\n",
    "            'Month': 6,\n",
    "            'Day': 10,\n",
    "            'State': 6})\n",
    "    \n",
    "    m.fit(X_train, y_train)\n",
    "    models.append(m)\n",
    "    print('\\n')\n",
    "\n",
    "test_y_pred = np.array([model.predict(X_test) for model in models])\n",
    "test_y_pred = test_y_pred.mean(axis=0)\n",
    "\n",
    "print('MAPE: %s' % eval_utils.MAPE(\n",
    "    y_true=y_test.values.flatten(),\n",
    "    y_pred=test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original output from repo code:\n",
    "    \n",
    "`\n",
    "Using TensorFlow backend.\n",
    "Number of samples used for training: 200000\n",
    "Fitting NN_with_EntityEmbedding...\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 64us/step - loss: 0.0142 - val_loss: 0.0119\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0096 - val_loss: 0.0109\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0089 - val_loss: 0.0113\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0082 - val_loss: 0.0101\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0077 - val_loss: 0.0101\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0074 - val_loss: 0.0100\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0072 - val_loss: 0.0099\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 59us/step - loss: 0.0071 - val_loss: 0.0096\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0069 - val_loss: 0.0092\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0068 - val_loss: 0.0095\n",
    "Result on validation data:  0.10152226095724903\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 63us/step - loss: 0.0140 - val_loss: 0.0117\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0093 - val_loss: 0.0107\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0084 - val_loss: 0.0109\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0079 - val_loss: 0.0096\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0076 - val_loss: 0.0097\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0074 - val_loss: 0.0097\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0072 - val_loss: 0.0097\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0093\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0069 - val_loss: 0.0094\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0068 - val_loss: 0.0093\n",
    "Result on validation data:  0.10194501967184522\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 64us/step - loss: 0.0141 - val_loss: 0.0121\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0093 - val_loss: 0.0100\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0084 - val_loss: 0.0098\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0079 - val_loss: 0.0095\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0076 - val_loss: 0.0098\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0074 - val_loss: 0.0097\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0072 - val_loss: 0.0098\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0071 - val_loss: 0.0092\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0093\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0069 - val_loss: 0.0097\n",
    "Result on validation data:  0.10076855799458961\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0141 - val_loss: 0.0114\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0093 - val_loss: 0.0105\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0084 - val_loss: 0.0108\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0079 - val_loss: 0.0099\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0076 - val_loss: 0.0098\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0074 - val_loss: 0.0099\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0072 - val_loss: 0.0100\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0071 - val_loss: 0.0095\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0096\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0068 - val_loss: 0.0099\n",
    "Result on validation data:  0.10973501886112967\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 63us/step - loss: 0.0144 - val_loss: 0.0116\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0094 - val_loss: 0.0109\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0084 - val_loss: 0.0103\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0079 - val_loss: 0.0099\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0076 - val_loss: 0.0104\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0074 - val_loss: 0.0099\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0072 - val_loss: 0.0099\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0099\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0069 - val_loss: 0.0097\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0068 - val_loss: 0.0100\n",
    "Result on validation data:  0.10491748954856149\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X_train, y_train, X_test, y_test = datasets.get_X_train_test_data()\n",
    "dtrain = xgb.DMatrix(\n",
    "    X_train.apply(lambda x: x.cat.codes),\n",
    "    label=np.log(y_train))\n",
    "evallist = [(dtrain, 'train')]\n",
    "param = {'nthread': 12,\n",
    "         'max_depth': 7,\n",
    "         'eta': 0.02,\n",
    "         'silent': 1,\n",
    "         'objective': 'reg:linear',\n",
    "         'colsample_bytree': 0.7,\n",
    "         'subsample': 0.7}\n",
    "\n",
    "num_round = 3000\n",
    "bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n",
    "\n",
    "xgb_test_y_pred = bst.predict(\n",
    "    xgb.DMatrix(X_test.apply(lambda x: x.cat.codes))\n",
    ")\n",
    "xgb_test_y_pred = np.exp((xgb_test_y_pred))\n",
    "print('MAPE: %s' % eval_utils.MAPE(\n",
    "    y_true=y_test, \n",
    "    y_pred=xgb_test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original output from repo code:\n",
    "\n",
    "`\n",
    ".\n",
    ".\n",
    ".\n",
    "[2987]  train-rmse:0.148847\n",
    "[2988]  train-rmse:0.148845\n",
    "[2989]  train-rmse:0.148842\n",
    "[2990]  train-rmse:0.148839\n",
    "[2991]  train-rmse:0.148834\n",
    "[2992]  train-rmse:0.148819\n",
    "[2993]  train-rmse:0.148768\n",
    "[2994]  train-rmse:0.148762\n",
    "[2995]  train-rmse:0.148741\n",
    "[2996]  train-rmse:0.148705\n",
    "[2997]  train-rmse:0.148667\n",
    "[2998]  train-rmse:0.148622\n",
    "[2999]  train-rmse:0.148584\n",
    "Result on validation data:  0.14691216270195093\n",
    "`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
