{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity-embedding-rossmann \n",
    "\n",
    "This is a Pytorch implementation with sklearn model interface.\n",
    "\n",
    "This reproduces the code used in the paper **\"[Entity Embeddings of Categorical Variables](http://arxiv.org/abs/1604.06737)\"**. \n",
    "\n",
    "Original version in Keras version can be found in: \n",
    "**Entity Embeddings of Categorical Variables [REPO](https://github.com/entron/entity-embedding-rossmann)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[1] Test: MSE:0.00018 MAE: 0.00939 gini: 0.90037 R2: 0.89092 MAPE: 0.01153\n",
      "\t[2] Test: MSE:0.00014 MAE: 0.00825 gini: 0.92758 R2: 0.91568 MAPE: 0.01012\n",
      "\t[3] Test: MSE:0.00013 MAE: 0.00786 gini: 0.93557 R2: 0.92246 MAPE: 0.00966\n",
      "\t[4] Test: MSE:0.00011 MAE: 0.00729 gini: 0.93948 R2: 0.93083 MAPE: 0.00895\n",
      "\t[5] Test: MSE:0.00011 MAE: 0.00734 gini: 0.94207 R2: 0.93115 MAPE: 0.00902\n",
      "\t[6] Test: MSE:0.00011 MAE: 0.00709 gini: 0.94487 R2: 0.93505 MAPE: 0.00871\n",
      "\t[7] Test: MSE:0.00011 MAE: 0.00714 gini: 0.93532 R2: 0.93431 MAPE: 0.00879\n",
      "\t[8] Test: MSE:0.0001 MAE: 0.00683 gini: 0.94437 R2: 0.93877 MAPE: 0.00839\n",
      "\t[9] Test: MSE:0.0001 MAE: 0.00663 gini: 0.95925 R2: 0.94113 MAPE: 0.00814\n",
      "\t[10] Test: MSE:9e-05 MAE: 0.00654 gini: 0.9411 R2: 0.94244 MAPE: 0.00802\n",
      "\n",
      "\n",
      "\t[1] Test: MSE:0.00018 MAE: 0.00954 gini: 0.90326 R2: 0.88809 MAPE: 0.01171\n",
      "\t[2] Test: MSE:0.00015 MAE: 0.00866 gini: 0.9222 R2: 0.90806 MAPE: 0.01064\n",
      "\t[3] Test: MSE:0.00013 MAE: 0.00801 gini: 0.93858 R2: 0.91997 MAPE: 0.00985\n",
      "\t[4] Test: MSE:0.00011 MAE: 0.00732 gini: 0.93974 R2: 0.93037 MAPE: 0.00897\n",
      "\t[5] Test: MSE:0.00014 MAE: 0.0086 gini: 0.95085 R2: 0.91388 MAPE: 0.01056\n",
      "\t[6] Test: MSE:0.00011 MAE: 0.00718 gini: 0.94448 R2: 0.93322 MAPE: 0.00882\n",
      "\t[7] Test: MSE:0.00011 MAE: 0.00705 gini: 0.9483 R2: 0.93516 MAPE: 0.00867\n",
      "\t[8] Test: MSE:0.0001 MAE: 0.00688 gini: 0.95069 R2: 0.93765 MAPE: 0.00845\n",
      "\t[9] Test: MSE:0.0001 MAE: 0.00673 gini: 0.92546 R2: 0.93959 MAPE: 0.00829\n",
      "\t[10] Test: MSE:9e-05 MAE: 0.00655 gini: 0.94907 R2: 0.94207 MAPE: 0.00805\n",
      "\n",
      "\n",
      "\t[1] Test: MSE:0.00018 MAE: 0.00953 gini: 0.90303 R2: 0.88811 MAPE: 0.01167\n",
      "\t[2] Test: MSE:0.00015 MAE: 0.00839 gini: 0.92067 R2: 0.90917 MAPE: 0.01032\n",
      "\t[3] Test: MSE:0.00013 MAE: 0.00775 gini: 0.93699 R2: 0.92255 MAPE: 0.00953\n",
      "\t[4] Test: MSE:0.00013 MAE: 0.00793 gini: 0.91284 R2: 0.92119 MAPE: 0.00977\n",
      "\t[5] Test: MSE:0.00011 MAE: 0.00736 gini: 0.92173 R2: 0.92963 MAPE: 0.00906\n",
      "\t[6] Test: MSE:0.00011 MAE: 0.00707 gini: 0.9244 R2: 0.93424 MAPE: 0.00869\n",
      "\t[7] Test: MSE:0.00011 MAE: 0.00716 gini: 0.92702 R2: 0.93429 MAPE: 0.00881\n",
      "\t[8] Test: MSE:0.0001 MAE: 0.0067 gini: 0.95261 R2: 0.94005 MAPE: 0.00823\n",
      "\t[9] Test: MSE:9e-05 MAE: 0.00653 gini: 0.9407 R2: 0.94226 MAPE: 0.00802\n",
      "\t[10] Test: MSE:9e-05 MAE: 0.00655 gini: 0.94767 R2: 0.94267 MAPE: 0.00802\n",
      "\n",
      "\n",
      "\t[1] Test: MSE:0.00018 MAE: 0.00971 gini: 0.90033 R2: 0.88663 MAPE: 0.01194\n",
      "\t[2] Test: MSE:0.00014 MAE: 0.00847 gini: 0.93348 R2: 0.9108 MAPE: 0.01042\n",
      "\t[3] Test: MSE:0.00012 MAE: 0.0076 gini: 0.93482 R2: 0.9261 MAPE: 0.00932\n",
      "\t[4] Test: MSE:0.00011 MAE: 0.00745 gini: 0.9374 R2: 0.9291 MAPE: 0.00916\n",
      "\t[5] Test: MSE:0.00013 MAE: 0.008 gini: 0.92729 R2: 0.92249 MAPE: 0.00986\n",
      "\t[6] Test: MSE:0.0001 MAE: 0.00691 gini: 0.94084 R2: 0.93694 MAPE: 0.00849\n",
      "\t[7] Test: MSE:0.0001 MAE: 0.00687 gini: 0.94664 R2: 0.93756 MAPE: 0.00844\n",
      "\t[8] Test: MSE:0.0001 MAE: 0.00688 gini: 0.94617 R2: 0.93778 MAPE: 0.00846\n",
      "\t[9] Test: MSE:9e-05 MAE: 0.00658 gini: 0.93554 R2: 0.94156 MAPE: 0.00809\n",
      "\t[10] Test: MSE:9e-05 MAE: 0.0065 gini: 0.96267 R2: 0.9426 MAPE: 0.00797\n",
      "\n",
      "\n",
      "\t[1] Test: MSE:0.0002 MAE: 0.01033 gini: 0.90943 R2: 0.87382 MAPE: 0.01271\n",
      "\t[2] Test: MSE:0.00015 MAE: 0.00846 gini: 0.92154 R2: 0.91024 MAPE: 0.01038\n",
      "\t[3] Test: MSE:0.00012 MAE: 0.00773 gini: 0.90986 R2: 0.92311 MAPE: 0.0095\n",
      "\t[4] Test: MSE:0.00011 MAE: 0.0073 gini: 0.92244 R2: 0.93035 MAPE: 0.00897\n",
      "\t[5] Test: MSE:0.00012 MAE: 0.00768 gini: 0.92714 R2: 0.92641 MAPE: 0.00946\n",
      "\t[6] Test: MSE:0.00011 MAE: 0.00712 gini: 0.94026 R2: 0.93408 MAPE: 0.00875\n",
      "\t[7] Test: MSE:0.0001 MAE: 0.00688 gini: 0.93737 R2: 0.9375 MAPE: 0.00846\n",
      "\t[8] Test: MSE:0.0001 MAE: 0.00669 gini: 0.9194 R2: 0.94 MAPE: 0.00823\n",
      "\t[9] Test: MSE:0.0001 MAE: 0.00694 gini: 0.92574 R2: 0.93676 MAPE: 0.00856\n",
      "\t[10] Test: MSE:9e-05 MAE: 0.00659 gini: 0.93111 R2: 0.94147 MAPE: 0.00811\n",
      "\n",
      "\n",
      "MAPE: 0.09784227344171176\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import eval_utils\n",
    "import numpy as np\n",
    "\n",
    "import EENN as eenn\n",
    "\n",
    "X, y, X_test, y_test = datasets.get_X_train_test_data()\n",
    "\n",
    "for data in [X, X_test]:\n",
    "    data.drop('Open', inplace=True, axis=1)\n",
    "\n",
    "models = []\n",
    "for _ in range(5):\n",
    "    m = eenn.EntEmbNNRegression(\n",
    "        cat_emb_dim={\n",
    "            'Store': 10,\n",
    "            'DayOfWeek': 6,\n",
    "            'Promo': 1,\n",
    "            'Year': 2,\n",
    "            'Month': 6,\n",
    "            'Day': 10,\n",
    "            'State': 6},\n",
    "        alpha=0,\n",
    "        dense_layers = [1000, 500],\n",
    "        drop_out_layers = [0., 0.],\n",
    "        drop_out_emb = 0.,\n",
    "        loss_function='L1Loss',\n",
    "        train_size = 1.,\n",
    "        y_max = max(y.max(), y_test.max())\n",
    "        )\n",
    "\n",
    "    m.fit(X, y)\n",
    "    models.append(m)\n",
    "    print('\\n')\n",
    "\n",
    "test_y_pred = np.array([model.predict(X_test) for model in models])\n",
    "test_y_pred = test_y_pred.mean(axis=0)\n",
    "\n",
    "print('MAPE: %s' % eval_utils.MAPE(\n",
    "    y_true=y_test.values.flatten(),\n",
    "    y_pred=test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Original output from [REPO](https://github.com/entron/entity-embedding-rossmann) code**:\n",
    "    \n",
    "`\n",
    "Using TensorFlow backend.\n",
    "Number of samples used for training: 200000\n",
    "Fitting NN_with_EntityEmbedding...\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 64us/step - loss: 0.0142 - val_loss: 0.0119\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0096 - val_loss: 0.0109\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0089 - val_loss: 0.0113\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0082 - val_loss: 0.0101\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0077 - val_loss: 0.0101\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0074 - val_loss: 0.0100\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0072 - val_loss: 0.0099\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 59us/step - loss: 0.0071 - val_loss: 0.0096\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0069 - val_loss: 0.0092\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0068 - val_loss: 0.0095\n",
    "Result on validation data:  0.10152226095724903\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 63us/step - loss: 0.0140 - val_loss: 0.0117\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0093 - val_loss: 0.0107\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0084 - val_loss: 0.0109\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0079 - val_loss: 0.0096\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0076 - val_loss: 0.0097\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0074 - val_loss: 0.0097\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0072 - val_loss: 0.0097\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0093\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0069 - val_loss: 0.0094\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0068 - val_loss: 0.0093\n",
    "Result on validation data:  0.10194501967184522\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 64us/step - loss: 0.0141 - val_loss: 0.0121\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0093 - val_loss: 0.0100\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0084 - val_loss: 0.0098\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0079 - val_loss: 0.0095\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0076 - val_loss: 0.0098\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0074 - val_loss: 0.0097\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0072 - val_loss: 0.0098\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0071 - val_loss: 0.0092\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0093\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0069 - val_loss: 0.0097\n",
    "Result on validation data:  0.10076855799458961\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0141 - val_loss: 0.0114\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0093 - val_loss: 0.0105\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0084 - val_loss: 0.0108\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0079 - val_loss: 0.0099\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0076 - val_loss: 0.0098\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 60us/step - loss: 0.0074 - val_loss: 0.0099\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0072 - val_loss: 0.0100\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0071 - val_loss: 0.0095\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0096\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0068 - val_loss: 0.0099\n",
    "Result on validation data:  0.10973501886112967\n",
    "Train on 200000 samples, validate on 84434 samples\n",
    "Epoch 1/10\n",
    "200000/200000 [==============================] - 13s 63us/step - loss: 0.0144 - val_loss: 0.0116\n",
    "Epoch 2/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0094 - val_loss: 0.0109\n",
    "Epoch 3/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0084 - val_loss: 0.0103\n",
    "Epoch 4/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0079 - val_loss: 0.0099\n",
    "Epoch 5/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0076 - val_loss: 0.0104\n",
    "Epoch 6/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0074 - val_loss: 0.0099\n",
    "Epoch 7/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0072 - val_loss: 0.0099\n",
    "Epoch 8/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0070 - val_loss: 0.0099\n",
    "Epoch 9/10\n",
    "200000/200000 [==============================] - 12s 62us/step - loss: 0.0069 - val_loss: 0.0097\n",
    "Epoch 10/10\n",
    "200000/200000 [==============================] - 12s 61us/step - loss: 0.0068 - val_loss: 0.0100\n",
    "Result on validation data:  0.10491748954856149\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGB performance**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.14955005536869034\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X_train, y_train, X_test, y_test = datasets.get_X_train_test_data()\n",
    "dtrain = xgb.DMatrix(\n",
    "    X_train.apply(lambda x: x.cat.codes),\n",
    "    label=np.log(y_train))\n",
    "evallist = [(dtrain, 'train')]\n",
    "param = {'nthread': 12,\n",
    "         'max_depth': 7,\n",
    "         'eta': 0.02,\n",
    "         'silent': 1,\n",
    "         'objective': 'reg:linear',\n",
    "         'colsample_bytree': 0.7,\n",
    "         'subsample': 0.7}\n",
    "\n",
    "num_round = 3000\n",
    "bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n",
    "\n",
    "xgb_test_y_pred = bst.predict(\n",
    "    xgb.DMatrix(X_test.apply(lambda x: x.cat.codes))\n",
    ")\n",
    "xgb_test_y_pred = np.exp((xgb_test_y_pred))\n",
    "print('MAPE: %s' % eval_utils.MAPE(\n",
    "    y_true=y_test, \n",
    "    y_pred=xgb_test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Original output from [REPO](https://github.com/entron/entity-embedding-rossmann) code**:\n",
    "\n",
    "`\n",
    ".\n",
    ".\n",
    ".\n",
    "[2987]  train-rmse:0.148847\n",
    "[2988]  train-rmse:0.148845\n",
    "[2989]  train-rmse:0.148842\n",
    "[2990]  train-rmse:0.148839\n",
    "[2991]  train-rmse:0.148834\n",
    "[2992]  train-rmse:0.148819\n",
    "[2993]  train-rmse:0.148768\n",
    "[2994]  train-rmse:0.148762\n",
    "[2995]  train-rmse:0.148741\n",
    "[2996]  train-rmse:0.148705\n",
    "[2997]  train-rmse:0.148667\n",
    "[2998]  train-rmse:0.148622\n",
    "[2999]  train-rmse:0.148584\n",
    "Result on validation data:  0.14691216270195093\n",
    "`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
